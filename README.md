# NLP Theresa Course Syllabus

## Master Program
THERESA - digiTal tecHnologiEs foR thE Security of criticAl infrastructures

## Course Description

This course provides a comprehensive introduction to Natural Language Processing (NLP), starting with the fundamentals and progressing to cutting-edge techniques like Large Language Models (LLMs). Students will learn about text preprocessing, vectorization, and traditional NLP tasks such as text classification and named entity recognition. The course then delves into deep learning methods, including RNNs, LSTMs, and the Transformer architecture, and how they're applied in NLP. The curriculum emphasizes hands-on experience with building and fine-tuning NLP models, as well as responsible AI practices. By the end of the course, students will have a solid understanding of both classical and modern NLP techniques and be able to apply them to real-world problems.

## Textbook

Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin, August 20, 2024 release [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)

## Topics

### 1. Introduction to NLP (8 hours)

*   **Overview of NLP:** Definition, key challenges, and real-world applications.
*   **Text Preprocessing:** Tokenization, stemming, lemmatization, stop word removal, and handling special characters.
*   **Text Vectorization Techniques:**
    *   Bag of Words (BoW)
    *   TF-IDF
    *   Word Embeddings (Word2Vec, GloVe, fastText)
*   **N-gram Language Model:** Understanding and implementing N-gram models.
*   **Text Classification Pipelines:** Building and evaluating text classification pipelines for sentiment analysis, topic categorization, and other tasks.
*   **Key NLP Tasks:**
    *   Text Classification
    *   Named Entity Recognition (NER) and its applications.

### 2. Deep Learning in NLP (8 hours)

*   **Introduction to ANNs:** Fundamentals of Artificial Neural Networks.
*   **Introduction to RNNs and LSTMs:** Recurrent Neural Networks and Long Short-Term Memory networks for sequence modeling in NLP.
*   **Neural Language Model (NLM):** Exploring neural language models and their advantages over traditional models.
*   **Attention Mechanisms and the Transformer Architecture:** Understanding the attention mechanism and the Transformer model.
*   **Transformer-Based Models (BERT, GPT, T5):** In-depth exploration of popular transformer models and their impact on NLP.
*   **Transfer Learning in NLP:** How to leverage pre-trained models for various NLP tasks.

### 3. LLMs in Depth (16 hours)

*   **Introduction to AI and LLMs:** Overview of Artificial Intelligence and Large Language Models.
*   **Types of LLMs:** Exploring different types of LLMs and their architectures.
*   **How LLMs work:** Understanding masked language models and other LLM techniques.
*   **Capabilities and Limitations of LLMs:** What LLMs can and cannot do effectively.
*   **Prompt Engineering:** Crafting effective prompts for various tasks and optimizing LLM performance.
*   **Choosing the Right LLM:** Key factors to consider (size, latency, accuracy).
*   **Evaluating LLMs:** Metrics for performance (accuracy, fluency, hallucinations, etc.).
*   **Retrieval Augmented Generation (RAG) with LLMs:** Combining LLMs with external knowledge sources.
*   **Introduction to Vector Databases for NLP Tasks:** Using vector databases for efficient similarity search and information retrieval.
*   **Responsible AI:** Ethical considerations and responsible use of LLMs.
*   **Small LLMs:** Lightweight models for specific tasks or devices (e.g., DistilBERT, TinyBERT).
*   **LLMs vs. SLMs:** Differences between Large Language Models and Small Language Models, and the role of SLMs in edge computing.
*   **Gen AI for AI Model Building:** Using Generative AI to build AI models instead of Classical AI.
*   **Gen AI for Synthetic Dataset Creation:** Leveraging Generative AI to create synthetic datasets for training and evaluation.

### 4. Practical Sessions: Implementing NLP Tasks (8 hours)

*   **Hands-on Text Classification and Sentiment Analysis:** Using pre-trained models and transformers.
*   **Fine-Tuning Transformer-Based Models:** Adapting pre-trained models for specific tasks.
*   **Demonstration of LLM-Based Applications:** Showcasing real-world applications of LLMs.
*   **Gen AI Tools in Academic Research:** Exploring the use of Generative AI tools in research, including image and video generation.
*   **Retrieval Augmented Generation (RAG) with LLMs:** Implementing RAG techniques for enhanced LLM performance.

## Assessment Criteria

*   Final Pass/Fail Exam 


# Copyright Notics 
These slides are distributed for educational purpose under the Creative Commons License. https://creativecommons.org/licenses/by-sa/2.0/legalcode

Source of Slides: 
* DeepLearning.AI as the source of the slides.
* Generative AI
* Harvard Teaching and Learning Consortium
