{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      1.00      1.00         1\n",
      "        spam       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample data: a small dataset of text messages\n",
    "data = {\n",
    "    'text': [\n",
    "        'Congratulations! You have won a lottery of $1000.',\n",
    "        'Click here to claim your prize.',\n",
    "        'Hey, how are you doing?',\n",
    "        'Don’t forget our meeting tomorrow at 10 AM.',\n",
    "        'You have a new message from your bank.',\n",
    "        'This is not spam, just a friendly reminder.',\n",
    "        'Win a free iPhone by clicking this link!',\n",
    "        'Let’s catch up sometime this week.'\n",
    "    ],\n",
    "    'label': [\n",
    "        'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Example with Larger Sample Data and Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Congratulations! You have won a lottery of $1000.</td>\n",
       "      <td>spam</td>\n",
       "      <td>congratulations you have won a lottery of 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Click here to claim your prize.</td>\n",
       "      <td>spam</td>\n",
       "      <td>click here to claim your prize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey, how are you doing?</td>\n",
       "      <td>ham</td>\n",
       "      <td>hey how are you doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Don’t forget our meeting tomorrow at 10 AM.</td>\n",
       "      <td>ham</td>\n",
       "      <td>don’t forget our meeting tomorrow at 10 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have a new message from your bank.</td>\n",
       "      <td>spam</td>\n",
       "      <td>you have a new message from your bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is not spam, just a friendly reminder.</td>\n",
       "      <td>ham</td>\n",
       "      <td>this is not spam just a friendly reminder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Win a free iPhone by clicking this link!</td>\n",
       "      <td>spam</td>\n",
       "      <td>win a free iphone by clicking this link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Let’s catch up sometime this week.</td>\n",
       "      <td>ham</td>\n",
       "      <td>let’s catch up sometime this week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Exclusive offer just for you! Buy now and save...</td>\n",
       "      <td>spam</td>\n",
       "      <td>exclusive offer just for you buy now and save big</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Your account has been compromised. Act now!</td>\n",
       "      <td>spam</td>\n",
       "      <td>your account has been compromised act now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This is a limited time offer. Don’t miss out!</td>\n",
       "      <td>spam</td>\n",
       "      <td>this is a limited time offer don’t miss out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hello! I hope you are having a great day.</td>\n",
       "      <td>ham</td>\n",
       "      <td>hello i hope you are having a great day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Your subscription will expire soon. Renew now.</td>\n",
       "      <td>spam</td>\n",
       "      <td>your subscription will expire soon renew now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>You have been selected for a special promotion.</td>\n",
       "      <td>ham</td>\n",
       "      <td>you have been selected for a special promotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Meet me at the coffee shop at 3 PM.</td>\n",
       "      <td>spam</td>\n",
       "      <td>meet me at the coffee shop at 3 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Important: Update your password immediately.</td>\n",
       "      <td>ham</td>\n",
       "      <td>important update your password immediately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Get paid to work from home! Sign up today.</td>\n",
       "      <td>spam</td>\n",
       "      <td>get paid to work from home sign up today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Reminder: Your appointment is tomorrow at 2 PM.</td>\n",
       "      <td>ham</td>\n",
       "      <td>reminder your appointment is tomorrow at 2 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Congratulations! You are our lucky winner!</td>\n",
       "      <td>spam</td>\n",
       "      <td>congratulations you are our lucky winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This is not spam, just checking in.</td>\n",
       "      <td>ham</td>\n",
       "      <td>this is not spam just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>You have a new message from your friend.</td>\n",
       "      <td>ham</td>\n",
       "      <td>you have a new message from your friend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label  \\\n",
       "0   Congratulations! You have won a lottery of $1000.  spam   \n",
       "1                     Click here to claim your prize.  spam   \n",
       "2                             Hey, how are you doing?   ham   \n",
       "3         Don’t forget our meeting tomorrow at 10 AM.   ham   \n",
       "4              You have a new message from your bank.  spam   \n",
       "5         This is not spam, just a friendly reminder.   ham   \n",
       "6            Win a free iPhone by clicking this link!  spam   \n",
       "7                  Let’s catch up sometime this week.   ham   \n",
       "8   Exclusive offer just for you! Buy now and save...  spam   \n",
       "9         Your account has been compromised. Act now!  spam   \n",
       "10      This is a limited time offer. Don’t miss out!  spam   \n",
       "11          Hello! I hope you are having a great day.   ham   \n",
       "12     Your subscription will expire soon. Renew now.  spam   \n",
       "13    You have been selected for a special promotion.   ham   \n",
       "14                Meet me at the coffee shop at 3 PM.  spam   \n",
       "15       Important: Update your password immediately.   ham   \n",
       "16         Get paid to work from home! Sign up today.  spam   \n",
       "17    Reminder: Your appointment is tomorrow at 2 PM.   ham   \n",
       "18         Congratulations! You are our lucky winner!  spam   \n",
       "19                This is not spam, just checking in.   ham   \n",
       "20           You have a new message from your friend.   ham   \n",
       "\n",
       "                                            text_norm  \n",
       "0      congratulations you have won a lottery of 1000  \n",
       "1                      click here to claim your prize  \n",
       "2                               hey how are you doing  \n",
       "3          don’t forget our meeting tomorrow at 10 am  \n",
       "4               you have a new message from your bank  \n",
       "5           this is not spam just a friendly reminder  \n",
       "6             win a free iphone by clicking this link  \n",
       "7                   let’s catch up sometime this week  \n",
       "8   exclusive offer just for you buy now and save big  \n",
       "9           your account has been compromised act now  \n",
       "10        this is a limited time offer don’t miss out  \n",
       "11            hello i hope you are having a great day  \n",
       "12       your subscription will expire soon renew now  \n",
       "13     you have been selected for a special promotion  \n",
       "14                 meet me at the coffee shop at 3 pm  \n",
       "15         important update your password immediately  \n",
       "16           get paid to work from home sign up today  \n",
       "17      reminder your appointment is tomorrow at 2 pm  \n",
       "18           congratulations you are our lucky winner  \n",
       "19                  this is not spam just checking in  \n",
       "20            you have a new message from your friend  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample data: a larger dataset of text messages\n",
    "data = {\n",
    "    'text': [\n",
    "        'Congratulations! You have won a lottery of $1000.',\n",
    "        'Click here to claim your prize.',\n",
    "        'Hey, how are you doing?',\n",
    "        'Don’t forget our meeting tomorrow at 10 AM.',\n",
    "        'You have a new message from your bank.',\n",
    "        'This is not spam, just a friendly reminder.',\n",
    "        'Win a free iPhone by clicking this link!',\n",
    "        'Let’s catch up sometime this week.',\n",
    "        'Exclusive offer just for you! Buy now and save big.',\n",
    "        'Your account has been compromised. Act now!',\n",
    "        'This is a limited time offer. Don’t miss out!',\n",
    "        'Hello! I hope you are having a great day.',\n",
    "        'Your subscription will expire soon. Renew now.',\n",
    "        'You have been selected for a special promotion.',\n",
    "        'Meet me at the coffee shop at 3 PM.',\n",
    "        'Important: Update your password immediately.',\n",
    "        'Get paid to work from home! Sign up today.',\n",
    "        'Reminder: Your appointment is tomorrow at 2 PM.',\n",
    "        'Congratulations! You are our lucky winner!',\n",
    "        'This is not spam, just checking in.',\n",
    "        'You have a new message from your friend.'\n",
    "    ],\n",
    "    'label': [\n",
    "        'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham',\n",
    "        'spam', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham',\n",
    "        'spam', 'ham', 'spam', 'ham', 'ham'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalization function\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "# Apply normalization to the text data\n",
    "df['text_norm'] = df['text'].apply(normalize_text)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.33      0.50         3\n",
      "        spam       0.60      1.00      0.75         3\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.80      0.67      0.62         6\n",
      "weighted avg       0.80      0.67      0.62         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_norm'], df['label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using the SMS Spam Collection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the ZIP archive: ['SMSSpamCollection', 'readme']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# Define the URL of the ZIP file containing the dataset\n",
    "zip_file_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "\n",
    "# Function to download and extract the dataset\n",
    "def load_sms_spam_collection(url):\n",
    "    # Download the ZIP file\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "\n",
    "    # Use BytesIO to read the ZIP file from the response content\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        # List all files in the ZIP\n",
    "        print(\"Files in the ZIP archive:\", z.namelist())\n",
    "        \n",
    "        # Specify the file to read\n",
    "        file_to_read = 'SMSSpamCollection'  # Change this to the desired file name\n",
    "\n",
    "        # Read the specific file from the ZIP\n",
    "        df = pd.read_csv(z.open(file_to_read), sep='\\t', names=['label', 'text'], encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the SMS Spam Collection Dataset\n",
    "df = load_sms_spam_collection(zip_file_url)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>will ü b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity  was in mood for that soany other suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   ham               Will ü b going to esplanade fr home?   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...   \n",
       "5571   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                              text_norm  \n",
       "0     go until jurong point crazy available only in ...  \n",
       "1                               ok lar joking wif u oni  \n",
       "2     free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3           u dun say so early hor u c already then say  \n",
       "4     nah i dont think he goes to usf he lives aroun...  \n",
       "...                                                 ...  \n",
       "5567  this is the 2nd time we have tried 2 contact u...  \n",
       "5568                will ü b going to esplanade fr home  \n",
       "5569  pity  was in mood for that soany other suggest...  \n",
       "5570  the guy did some bitching but i acted like id ...  \n",
       "5571                          rofl its true to its name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "df['text_norm'] = df['text'].str.replace('[{}]'.format(string.punctuation), '', regex=True).str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.98       966\n",
      "        spam       1.00      0.68      0.81       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.98      0.84      0.89      1115\n",
      "weighted avg       0.96      0.96      0.95      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_norm'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am very happy with this product.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Sentence: This is the worst purchase I have ever made.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "Sentence: Absolutely fantastic, highly recommend it.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Sentence: I am so disappointed, this is terrible.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "Sentence: This is a great product, very satisfied.\n",
      "Predicted Sentiment: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "\n",
    "# Generate labeled training data\n",
    "training_data = [\n",
    "    (\"I love this product, it is amazing!\", \"positive\"),\n",
    "    (\"This is the best thing I have ever bought.\", \"positive\"),\n",
    "    (\"Absolutely fantastic, I am very happy with it.\", \"positive\"),\n",
    "    (\"I am so pleased with this purchase.\", \"positive\"),\n",
    "    (\"This is wonderful, I highly recommend it.\", \"positive\"),\n",
    "    (\"I am extremely satisfied with this.\", \"positive\"),\n",
    "    (\"This is a great product, I will buy it again.\", \"positive\"),\n",
    "    (\"I am very impressed with the quality.\", \"positive\"),\n",
    "    (\"This is exactly what I needed, perfect!\", \"positive\"),\n",
    "    (\"I am delighted with this item.\", \"positive\"),\n",
    "    (\"I hate this product, it is terrible.\", \"negative\"),\n",
    "    (\"This is the worst thing I have ever bought.\", \"negative\"),\n",
    "    (\"Absolutely awful, I am very unhappy with it.\", \"negative\"),\n",
    "    (\"I am so disappointed with this purchase.\", \"negative\"),\n",
    "    (\"This is horrible, I do not recommend it.\", \"negative\"),\n",
    "    (\"I am extremely dissatisfied with this.\", \"negative\"),\n",
    "    (\"This is a terrible product, I will never buy it again.\", \"negative\"),\n",
    "    (\"I am very unimpressed with the quality.\", \"negative\"),\n",
    "    (\"This is not what I needed, awful!\", \"negative\"),\n",
    "    (\"I am disgusted with this item.\", \"negative\")\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.classes = defaultdict(lambda: 0)\n",
    "        self.word_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.class_totals = defaultdict(lambda: 0)\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, data):\n",
    "        for sentence, label in data:\n",
    "            self.classes[label] += 1\n",
    "            words = preprocess(sentence).split()\n",
    "            for word in words:\n",
    "                self.word_counts[label][word] += 1\n",
    "                self.class_totals[label] += 1\n",
    "                self.vocab.add(word)\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        words = preprocess(sentence).split()\n",
    "        class_scores = {}\n",
    "        for label in self.classes:\n",
    "            class_scores[label] = log(self.classes[label] / sum(self.classes.values()))\n",
    "            for word in words:\n",
    "                word_count = self.word_counts[label][word] + 1  # Laplace smoothing\n",
    "                class_scores[label] += log(word_count / (self.class_totals[label] + len(self.vocab)))\n",
    "        return max(class_scores, key=class_scores.get)\n",
    "\n",
    "# Initialize and train the classifier\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(training_data)\n",
    "\n",
    "# Test the classifier on 5 sentences\n",
    "test_sentences = [\n",
    "    \"I am very happy with this product.\",\n",
    "    \"This is the worst purchase I have ever made.\",\n",
    "    \"Absolutely fantastic, highly recommend it.\",\n",
    "    \"I am so disappointed, this is terrible.\",\n",
    "    \"This is a great product, very satisfied.\"\n",
    "]\n",
    "\n",
    "# Predict and print the results\n",
    "for sentence in test_sentences:\n",
    "    prediction = classifier.predict(sentence)\n",
    "    print(f\"Sentence: {sentence}\\nPredicted Sentiment: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: quality., Positive: 1, Negative: 1\n",
      "Word: wonderful,, Positive: 1, Negative: 0\n",
      "Word: again., Positive: 1, Negative: 1\n",
      "Word: terrible., Positive: 0, Negative: 1\n",
      "Word: exactly, Positive: 1, Negative: 0\n",
      "Word: will, Positive: 1, Negative: 1\n",
      "Word: pleased, Positive: 1, Negative: 0\n",
      "Word: product,, Positive: 2, Negative: 2\n",
      "Word: best, Positive: 1, Negative: 0\n",
      "Word: not, Positive: 0, Negative: 2\n",
      "Word: satisfied, Positive: 1, Negative: 0\n",
      "Word: terrible, Positive: 0, Negative: 1\n",
      "Word: extremely, Positive: 1, Negative: 1\n",
      "Word: have, Positive: 1, Negative: 1\n",
      "Word: highly, Positive: 1, Negative: 0\n",
      "Word: great, Positive: 1, Negative: 0\n",
      "Word: awful!, Positive: 0, Negative: 1\n",
      "Word: ., Positive: 2, Negative: 2\n",
      "Word: needed,, Positive: 1, Negative: 1\n",
      "Word: ever, Positive: 1, Negative: 1\n",
      "Word: recommend, Positive: 1, Negative: 1\n",
      "Word: hate, Positive: 0, Negative: 1\n",
      "Word: purchase., Positive: 1, Negative: 1\n",
      "Word: disgusted, Positive: 0, Negative: 1\n",
      "Word: with, Positive: 5, Negative: 5\n",
      "Word: horrible,, Positive: 0, Negative: 1\n",
      "Word: this, Positive: 7, Negative: 7\n",
      "Word: perfect!, Positive: 1, Negative: 0\n",
      "Word: worst, Positive: 0, Negative: 1\n",
      "Word: thing, Positive: 1, Negative: 1\n",
      "Word: unimpressed, Positive: 0, Negative: 1\n",
      "Word: the, Positive: 2, Negative: 2\n",
      "Word: absolutely, Positive: 1, Negative: 1\n",
      "Word: what, Positive: 1, Negative: 1\n",
      "Word: love, Positive: 1, Negative: 0\n",
      "Word: dissatisfied, Positive: 0, Negative: 1\n",
      "Word: unhappy, Positive: 0, Negative: 1\n",
      "Word: buy, Positive: 1, Negative: 1\n",
      "Word: this., Positive: 1, Negative: 1\n",
      "Word: item., Positive: 1, Negative: 1\n",
      "Word: fantastic,, Positive: 1, Negative: 0\n",
      "Word: impressed, Positive: 1, Negative: 0\n",
      "Word: delighted, Positive: 1, Negative: 0\n",
      "Word: never, Positive: 0, Negative: 1\n",
      "Word: amazing!, Positive: 1, Negative: 0\n",
      "Word: very, Positive: 2, Negative: 2\n",
      "Word: disappointed, Positive: 0, Negative: 1\n",
      "Word: awful,, Positive: 0, Negative: 1\n",
      "Word: bought., Positive: 1, Negative: 1\n",
      "Word: happy, Positive: 1, Negative: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize dictionaries to hold word counts for positive and negative sentences\n",
    "positive_word_counts = defaultdict(int)\n",
    "negative_word_counts = defaultdict(int)\n",
    "\n",
    "# Count the words in positive and negative sentences\n",
    "for sentence, sentiment in training_data:\n",
    "    words = preprocess(sentence).split()\n",
    "    if sentiment == 'positive':\n",
    "        for word in words:\n",
    "            positive_word_counts[word] += 1\n",
    "    else:\n",
    "        for word in words:\n",
    "            negative_word_counts[word] += 1\n",
    "\n",
    "# Combine the counts into a single dictionary for display\n",
    "word_counts = {}\n",
    "for word in set(positive_word_counts.keys()).union(set(negative_word_counts.keys())):\n",
    "    word_counts[word] = {\n",
    "        'positive': positive_word_counts[word],\n",
    "        'negative': negative_word_counts[word]\n",
    "    }\n",
    "\n",
    "# Display the word counts\n",
    "for word, counts in word_counts.items():\n",
    "    print(f\"Word: {word}, Positive: {counts['positive']}, Negative: {counts['negative']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
