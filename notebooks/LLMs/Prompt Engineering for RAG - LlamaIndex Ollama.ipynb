{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering for RAG\n",
    "\n",
    "\n",
    "In this notebook we show various prompt techniques you can try to customize your LlamaIndex RAG pipeline.\n",
    "\n",
    "* Getting and setting prompts for query engines, etc.\n",
    "* Defining template variable mappings (e.g. you have an existing QA prompt)\n",
    "* Adding few-shot examples + performing query transformations/rewriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-ollama -q \n",
    "%pip install llama-index-llms-langchain -q \n",
    "%pip install llama-index-readers-file pymupdf -q\n",
    "%pip install llama-index -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import PromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Settings.llm = Ollama(model=\"llama3.2:1b\")\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text:latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Download the PDF file\n",
    "url = \"https://arxiv.org/pdf/2307.09288.pdf\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "with urllib.request.urlopen(req) as response, open('data/llama2.pdf', 'wb') as out_file:\n",
    "    out_file.write(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load into Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama1b_llm = Ollama(model=\"llama3.2:1b\")\n",
    "llama3b_llm = Ollama(model=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Query Engine / Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"What are the potential risks associated with the use of Llama 2 as mentioned in the context?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=2, llm=llama1b_llm)\n",
    "# use this for testing\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The potential risks associated with the use of Llama 2 include:\n",
      "\n",
      "1. Inaccurate or objectionable responses to user prompts due to its language model nature and lack of human oversight.\n",
      "2. Limited understanding of certain cultural, social, or domain-specific nuances that may be present in user input.\n",
      "3. Potential for Llama 2 to generate content that is misleading, defamatory, or otherwise harmful to individuals, organizations, or communities.\n",
      "4. Dependence on high-quality training data and fine-tuning with human feedback to mitigate risks, but potential gaps in coverage or updated models if not regularly updated.\n",
      "\n",
      "These risks are highlighted in the context as a result of testing Llama 2 in English only, which may not cover all scenarios and requires developers to perform safety testing and tuning tailored to specific applications.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing/Customizing Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}\" f\"**Text:** \"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load prompt templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# display prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize Prompts\n",
    "\n",
    "What if we want to do something different than our standard question-answering prompts?\n",
    "\n",
    "Let's try out the RAG prompt from LangchainHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\motaz\\anaconda3\\envs\\nlp\\Lib\\site-packages\\langsmith\\client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# to do this, you need to use the langchain object\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "langchain_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One catch is that the template variables in the prompt are different than what's expected by our synthesizer in the query engine:\n",
    "\n",
    "* the prompt uses context and question,\n",
    "* we expect context_str and query_str\n",
    "\n",
    "This is not a problem! Let's add our template variable mappings to map variables. We use our LangchainPromptTemplate to map to LangChain prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import LangchainPromptTemplate\n",
    "\n",
    "lc_prompt_tmpl = LangchainPromptTemplate(\n",
    "    template=langchain_prompt,\n",
    "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
    ")\n",
    "\n",
    "# add langchain prompt templates to llamaIndex templates\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": lc_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try It Out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The potential risks associated with the use of Llama 2 include:\n",
      "\n",
      "* Inaccurate or objectionable responses to user prompts\n",
      "* Potential for generating text that is offensive, discriminatory, or violates applicable laws and regulations.\n",
      "\n",
      "Specifically, testing conducted on English has not covered all scenarios, and developers should perform safety testing and tuning tailored to their specific applications.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Few-Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "few_shot_nodes = []\n",
    "for line in open(\"llama2_qa_citation_events.jsonl\", \"r\"):\n",
    "    few_shot_nodes.append(TextNode(text=line))\n",
    "\n",
    "few_shot_index = VectorStoreIndex(few_shot_nodes)\n",
    "few_shot_retriever = few_shot_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def few_shot_examples_fn(**kwargs):\n",
    "    query_str = kwargs[\"query_str\"]\n",
    "    retrieved_nodes = few_shot_retriever.retrieve(query_str)\n",
    "    # go through each node, get json object\n",
    "\n",
    "    result_strs = []\n",
    "    for n in retrieved_nodes:\n",
    "        raw_dict = json.loads(n.get_content())\n",
    "        query = raw_dict[\"query\"]\n",
    "        response_dict = json.loads(raw_dict[\"response\"])\n",
    "        result_str = f\"\"\"\\\n",
    "Query: {query}\n",
    "Response: {response_dict}\"\"\"\n",
    "        result_strs.append(result_str)\n",
    "    return \"\\n\\n\".join(result_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write prompt template with functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write prompt template with functions\n",
    "\n",
    "qa_prompt_tmpl_str = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, \\\n",
    "answer the query asking about citations over different topics.\n",
    "Please provide your answer in the form of a structured JSON format containing \\\n",
    "a list of authors as the citations. Some examples are given below.\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(\n",
    "    qa_prompt_tmpl_str,\n",
    "    function_mappings={\"few_shot_examples\": few_shot_examples_fn},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_query_str = (\n",
    "    \"Which citations are mentioned in the section on Safety RLHF?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "test_context\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query asking about citations over different topics.\n",
      "Please provide your answer in the form of a structured JSON format containing a list of authors as the citations. Some examples are given below.\n",
      "\n",
      "Query: Which citations are mentioned in the section on RLHF Results?\n",
      "Response: {'citations': [{'author': 'Gilardi et al.', 'year': 2023, 'desc': ''}, {'author': 'Huang et al.', 'year': 2023, 'desc': ''}]}\n",
      "\n",
      "Query: Which citations are related to the progression of SFT and RLHF versions?\n",
      "Response: {'citations': [{'author': 'Gilardi et al.', 'year': 2023, 'desc': 'Documented the superior writing abilities of LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF'}, {'author': 'Huang et al.', 'year': 2023, 'desc': 'Supported the findings of Gilardi et al. on the effectiveness of RLHF in driving the superior writing abilities of LLMs'}]}\n",
      "\n",
      "Query: Which citations are mentioned in the section on Safety RLHF?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    qa_prompt_tmpl.format(\n",
    "        query_str=citation_query_str, context_str=\"test_context\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query asking about citations over different topics.\n",
      "Please provide your answer in the form of a structured JSON format containing a list of authors as the citations. Some examples are given below.\n",
      "\n",
      "{few_shot_examples}\n",
      "\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"citations\": [\n",
      "  {\n",
      "   \"author\": \"Gilardi et al.\",\n",
      "   \"year\": 2023,\n",
      "   \"desc\": \"Documented the superior writing abilities of LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF\"\n",
      "  },\n",
      "  {\n",
      "   \"author\": \"Huang et al.\",\n",
      "   \"year\": 2023,\n",
      "   \"desc\": \"Supported the findings of Gilardi et al. on the effectiveness of RLHF in driving the superior writing abilities of LLMs\"\n",
      "  }\n",
      " ]\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(citation_query_str)\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
